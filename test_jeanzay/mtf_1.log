2021-04-02 10:41:12.660569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
full_path =  /linkhome/idris/genhpe/shpe033/.conda/envs/mtf/lib/python3.8/site-packages/horovod-0.21.1-py3.8-linux-x86_64.egg/horovod/tensorflow/mpi_lib.cpython-38-x86_64-linux-gnu.so
self.BOOST_NCCL_LIB_CTYPES is created.

Horovod/tensorflow/__init__.py, Before import alltoall.


Horovod/tensorflow/__init__.py, After import alltoall.

[2021-04-02 10:41:22.164713: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1414] horovod_init(), nranks = 0
[2021-04-02 10:41:22.164852: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/utils/env_parser.cc:107] Using MPI to perform controller operations.
[2021-04-02 10:41:22.164931: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/utils/env_parser.cc:73] Using MPI to perform CPU operations.
[2021-04-02 10:41:22.165195: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:64] [Controller::Controller, response_cache.old_capacity = 0]: 
[2021-04-02 10:41:22.165275: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:66] [Controller::Controller, response_cache.capacity = 256]: 
[2021-04-02 10:41:22.165382: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:68] [Controller::Controller, response_cache_.capacity = 256]: 
[2021-04-02 10:41:22.165460: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:70] [Controller::Controller, after reset, response_cache.capacity = 0]: 
[2021-04-02 10:41:22.165538: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.h:34] MPIController::MPIController() , MPI Controller Initialized.
[2021-04-02 10:41:22.165637: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.h:46] MPI context enabled.
[2021-04-02 10:41:22.165724: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:93] [MPIContext::Initialize() entered, ranks.size = 0]: 
[2021-04-02 10:41:22.165814: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:175] [MPIContext::Initialize() mpi_comm == MPI_COMM_NULL, Using MPI_COMM_WORLD as a communicator.]: 
[2021-04-02 10:41:22.166086: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:181] [MPIContext::Initialize() mpi_comm == MPI_COMM_NULL, Set WorldMpiComm, mpi_comm_ptr and cookie.]: 
[2021-04-02 10:41:22.166254: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:196] [MPI_Comm_split local_comm OK]: 
[2021-04-02 10:41:22.166345: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:202] MPIContext::Initialize() world_rank = 1, local_rank = 1
[2021-04-02 10:41:22.166495: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:208] [MPI_Comm_split cross_comm OK]: 
[2021-04-02 10:41:22.166588: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1307] [InitializeHorovodOnce, MPIController[0] initialized and enabled]: 
[2021-04-02 10:41:22.166671: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1316] [InitializeHorovodOnce, nranks for controller 0 = ]: 4
[2021-04-02 10:41:22.166752: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1323] InitializeHorovodOnce(), Pushed process_group WORLD_COMM  into nccl_context.nccl_comms.
[2021-04-02 10:41:22.166836: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:38] [MPIController::DoInitialization(), entered.]: 
[2021-04-02 10:41:22.166932: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:49] MPIController::DoInitialization(), Not a Coordinator : Starting Horovod with 4 processes
[2021-04-02 10:41:22.276272: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:92] [MPIController::DoInitialization() ended.]: 
[2021-04-02 10:41:22.276404: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1326] [InitializeHorovodOnce(), Controller[0] initialized]: 
[2021-04-02 10:41:22.276488: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1328] [InitializeHorovodOnce(), First part of init done.]: 
[2021-04-02 10:41:22.276587: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1336] [InitializeHorovodOnce(), in TEST , NOT is_coordinator, ret_code = 0, retbuf = -1336688000]: 
[2021-04-02 10:41:22.276743: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1338] [InitializeHorovodOnce(), in TEST , NOT is_coordinator : (mpi_ctx_.mpi_comm == horovod::common::GetMpiWorldComm() = 1]: 
[2021-04-02 10:41:22.276828: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1355] [InitializeHorovodOnce(), First part of init done, TEST done]: 

nccl_create_process_groups in basics.py entered

nccl_create_process_groups in basics.py before import

nccl_create_process_groups in basics.py after import

nccl_create_process_groups in basics.py created self.boost_nccl_object.
boost_nccl::create_process_groups() entered **************
 ******* vecvec[0, 0] => 1 *******
 ******* vecvec[0, 1] => 2 *******
 ******* vecvec[1, 0] => 2 *******
 ******* vecvec[1, 1] => 3 *******
 ******* vecvec[1, 2] => 0 *******
boost_nccl::create_process_groups() calling horovod_nccl_create_process_groups()
[2021-04-02 10:41:22.277546: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1616] horovod_nccl_create_process_groups() entered. Nb of process_groups = 2
[2021-04-02 10:41:22.277629: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1623] [horovod_nccl_create_process_groups() ,rank_ 1 found in process_group 0]: 
[2021-04-02 10:41:22.277700: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1634]  horovod_nccl_create_process_groups(), Pushed process_group 0 into nccl_context.nccl_comms.
[2021-04-02 10:41:22.277780: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1644] [horovod_nccl_create_process_groups(), before new MPIController()]: 
[2021-04-02 10:41:22.277855: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:64] [Controller::Controller, response_cache.old_capacity = 0]: 
[2021-04-02 10:41:22.277940: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:66] [Controller::Controller, response_cache.capacity = 256]: 
[2021-04-02 10:41:22.278056: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:68] [Controller::Controller, response_cache_.capacity = 256]: 
[2021-04-02 10:41:22.278136: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:70] [Controller::Controller, after reset, response_cache.capacity = 0]: 
[2021-04-02 10:41:22.278214: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.h:34] MPIController::MPIController() , MPI Controller Initialized.
[2021-04-02 10:41:22.278292: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1653] [horovod_nccl_create_process_groups(), after new MPIController()]: 
[2021-04-02 10:41:22.278389: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1658] [horovod_nccl_create_process_groups(), controller[0] ]: 
[2021-04-02 10:41:22.278473: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1660] [horovod_nccl_create_process_groups(), controller[ii] = 1. ]: 
[2021-04-02 10:41:22.278548: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1660] [horovod_nccl_create_process_groups(), controller[ii] = 2. ]: 
[2021-04-02 10:41:22.278628: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1664] [horovod_nccl_create_process_groups() , index = 1, size = 2]: 
[2021-04-02 10:41:22.278729: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1625] [horovod_nccl_create_process_groups() ,rank_ 1 NOT found in process_group 1 => DOES NOT CREATE the MPIController nor NCCL_COMM]: 
[2021-04-02 10:41:22.278834: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1670] [horovod_nccl_create_process_groups() , after creation loop, before creating thread, j=0, k=0, horovod_global.process_groups[j][k]=0]: 
[2021-04-02 10:41:22.278915: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1670] [horovod_nccl_create_process_groups() , after creation loop, before creating thread, j=0, k=1, horovod_global.process_groups[j][k]=1]: 
[2021-04-02 10:41:22.279001: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1670] [horovod_nccl_create_process_groups() , after creation loop, before creating thread, j=0, k=2, horovod_global.process_groups[j][k]=2]: 
[2021-04-02 10:41:22.279084: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1670] [horovod_nccl_create_process_groups() , after creation loop, before creating thread, j=0, k=3, horovod_global.process_groups[j][k]=3]: 
[2021-04-02 10:41:22.279166: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1670] [horovod_nccl_create_process_groups() , after creation loop, before creating thread, j=1, k=0, horovod_global.process_groups[j][k]=1]: 
[2021-04-02 10:41:22.279248: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1670] [horovod_nccl_create_process_groups() , after creation loop, before creating thread, j=1, k=1, horovod_global.process_groups[j][k]=2]: 
[2021-04-02 10:41:22.279448: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:608] [BackgroundThreadLoop(), Start.]: 
[2021-04-02 10:41:22.279552: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:624] [BackgroundThreadLoop(), after creating MPIContextManager()]: 
[2021-04-02 10:41:22.279631: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:639] [BackgroundThreadLoop(), before state.num_nccl_streams.]: 
[2021-04-02 10:41:22.279725: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:649] [BackgroundThreadLoop(), state.num_nccl_streams from getenv = ]: 2
[2021-04-02 10:41:22.279882: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:663] [BackgroundThreadLoop(), before parameter_manager.]: 
[2021-04-02 10:41:22.279984: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:691] [BackgroundThreadLoop(), for n = 0, state.cache_capacity = 1024, parameter_manager.CacheEnabled = 1]: 
[2021-04-02 10:41:22.280073: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:695] [BackgroundThreadLoop(), for n = 0, state.response_cache[n].capacity = 1024]: 
[2021-04-02 10:41:22.280164: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:698] [BackgroundThreadLoop(), for n = 0, state.controller[n].response_cache_.capacity = 1024]: 
[2021-04-02 10:41:22.280250: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:701] [BackgroundThreadLoop(), for n = 0, Directly : state.controller[n].response_cache_.capacity = 1024]: 
[2021-04-02 10:41:22.280329: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:691] [BackgroundThreadLoop(), for n = 1, state.cache_capacity = 1024, parameter_manager.CacheEnabled = 1]: 
[2021-04-02 10:41:22.280413: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:695] [BackgroundThreadLoop(), for n = 1, state.response_cache[n].capacity = 1024]: 
[2021-04-02 10:41:22.280490: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:698] [BackgroundThreadLoop(), for n = 1, state.controller[n].response_cache_.capacity = 1024]: 
[2021-04-02 10:41:22.280573: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:701] [BackgroundThreadLoop(), for n = 1, Directly : state.controller[n].response_cache_.capacity = 1024]: 
[2021-04-02 10:41:22.280652: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:723] [BackgroundThreadLoop(), initialization loop on num_nccl_streams start]: 
[2021-04-02 10:41:22.280733: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:727] [BackgroundThreadLoop(), stream no = 0]: 
[2021-04-02 10:41:22.280816: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:728] [BackgroundThreadLoop(), ranks : ]: 
[2021-04-02 10:41:22.280905: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:730] [BackgroundThreadLoop(), ranks : 0]: 
[2021-04-02 10:41:22.281003: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:730] [BackgroundThreadLoop(), ranks : 1]: 
[2021-04-02 10:41:22.281078: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:730] [BackgroundThreadLoop(), ranks : 2]: 
[2021-04-02 10:41:22.281153: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:730] [BackgroundThreadLoop(), ranks : 3]: 
[2021-04-02 10:41:22.281236: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:727] [BackgroundThreadLoop(), stream no = 1]: 
[2021-04-02 10:41:22.281314: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:728] [BackgroundThreadLoop(), ranks : ]: 
[2021-04-02 10:41:22.281392: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:730] [BackgroundThreadLoop(), ranks : 1]: 
[2021-04-02 10:41:22.281469: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:730] [BackgroundThreadLoop(), ranks : 2]: 
[2021-04-02 10:41:22.281609: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:743] [BackgroundThreadLoop(), This rank : 1 was found in this Communicator : 1]: 
[2021-04-02 10:41:22.281686: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.h:46] MPI context enabled.
[2021-04-02 10:41:22.281770: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:93] [MPIContext::Initialize() entered, ranks.size = 2]: 
[2021-04-02 10:41:22.281848: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:95] [MPIContext::Initialize() ranks[0] = 1]: 
[2021-04-02 10:41:22.281927: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:95] [MPIContext::Initialize() ranks[1] = 2]: 
[2021-04-02 10:41:22.282015: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:136] MPIContext::Initialize()  ranks is NOT empty.
[2021-04-02 10:41:22.282092: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:138] [MPI_Group ranks[0] = 1]: 
[2021-04-02 10:41:22.282174: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:138] [MPI_Group ranks[1] = 2]: 
[2021-04-02 10:41:22.282266: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:147] [MPI_Group_incl OK]: 
[2021-04-02 10:41:22.284180: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:155] [MPI_Comm_create_group OK]: 
[2021-04-02 10:41:22.284261: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:162] [MPIContext::Initialize()  MPI_Group created.]: 
[2021-04-02 10:41:22.284397: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:196] [MPI_Comm_split local_comm OK]: 
[2021-04-02 10:41:22.284473: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:202] MPIContext::Initialize() world_rank = 0, local_rank = 0
[2021-04-02 10:41:22.284586: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_context.cc:208] [MPI_Comm_split cross_comm OK]: 
[2021-04-02 10:41:22.284667: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:748] [BackgroundThreadLoop(), mpi_ctx_ initialized for MPI_controller : 1]: 
[2021-04-02 10:41:22.284740: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:38] [MPIController::DoInitialization(), entered.]: 
[2021-04-02 10:41:22.284813: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:47] MPIController::DoInitialization(), Coordinator : Starting Horovod with 2 processes
[2021-04-02 10:41:22.284922: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:92] [MPIController::DoInitialization() ended.]: 
[2021-04-02 10:41:22.284999: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:752] [BackgroundThreadLoop(), Controller[] initialized() for : 1]: 
[2021-04-02 10:41:22.285077: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/timeline.cc:73] Setting TimelineFile. Current file: New filename:
[2021-04-02 10:41:22.285147: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/timeline.cc:105] Inited TimelineWriter but active_ is false, since filename passed is empty string
[2021-04-02 10:41:22.285258: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:834] [BackgroundThreadLoop(), initialization loop on num_nccl_streams end]: 
[2021-04-02 10:41:22.285332: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:844] [BackgroundThreadLoop(), calling CreateOperationManager for controller : 0]: 
[2021-04-02 10:41:22.285405: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:172] [CreateOperationManager(), entered with iComm = 0]: 
[2021-04-02 10:41:22.285515: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.285608: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.285693: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.285773: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.285845: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.285919: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.285996: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:232] [CreateOperationManager(), pushed NCCLAlltoall into alltoall_ops]: 
[2021-04-02 10:41:22.286070: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:288] [CreateOperationManager(), before calling new OperationManager]: 
[2021-04-02 10:41:22.286160: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:848] [BackgroundThreadLoop(), returned from CreateOperationManager for controller : 0]: 
[2021-04-02 10:41:22.286234: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:844] [BackgroundThreadLoop(), calling CreateOperationManager for controller : 1]: 
[2021-04-02 10:41:22.286307: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:172] [CreateOperationManager(), entered with iComm = 1]: 
[2021-04-02 10:41:22.286391: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.286462: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.286540: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.286621: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.286675: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.286763: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:31] [GPUOpContext::GPUOpContext() entered.]: 
[2021-04-02 10:41:22.286850: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:232] [CreateOperationManager(), pushed NCCLAlltoall into alltoall_ops]: 
[2021-04-02 10:41:22.286910: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:288] [CreateOperationManager(), before calling new OperationManager]: 
[2021-04-02 10:41:22.286995: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:848] [BackgroundThreadLoop(), returned from CreateOperationManager for controller : 1]: 
[2021-04-02 10:41:22.287078: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:854] [1 : Horovod Initialized for this rank]: 
[2021-04-02 10:41:22.287811: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1680] [horovod_nccl_create_process_groups() ,BackgroundThreadLoop created]: 
boost_nccl::create_process_groups() called horovod_nccl_create_process_groups()
nccl_create_process_groups in basics.py ended.


PYTHON : Tensor before alltoall =  [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [1.1, 1.1, 1.1, 1.1, 1.1, 1.1], [2.1, 2.1, 2.1, 2.1, 2.1, 2.1], [3.3, 3.3, 3.3, 3.3, 3.3, 3.3]]
tensorflow/mpi_ops.py , alltoall() Start, process_group = 0
2021-04-02 10:41:22.288249: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-02 10:41:22.288325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-02 10:41:22.289737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2021-04-02 10:41:22.291023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:1c:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2021-04-02 10:41:22.295926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: 
pciBusID: 0000:88:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2021-04-02 10:41:22.300854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: 
pciBusID: 0000:8a:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2021-04-02 10:41:22.300887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-02 10:41:22.301440: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfswork/idris/hpe/shpe033/horovod-v0.21.1/horovod/tensorflow:/gpfslocalsup/spack_soft/boost/1.70.0/gcc-8.3.1-ac7s6bl2rvxgv7kt5dhqujijptqob26c/lib:/gpfswork/idris/hpe/shpe033/meshtensorflow_project/boost_nccl:/gpfswork/idris/hpe/shpe033/meshtensorflow_project/install_env/cuda:/gpfslocalsup/spack_soft/openmpi/4.0.2/intel-19.0.4-q6fk6qb3intsc3raxyvu6x3as6uadzsl/lib:/gpfslocalsup/spack_soft/cudnn/7.6.5.32-10.1-linux-x64/gcc-4.8.5-fn27wz3xidimpfcu4t3ctvc6vxjr3afy/lib64:/gpfslocalsup/spack_soft/nccl/2.5.6-2/gcc-4.8.5-qtdldqth7z3ybxfozhgrjryw6c2ideaw/lib:/gpfslocalsys/cuda/10.1.2/nvvm/lib64:/gpfslocalsys/cuda/10.1.2/extras/CUPTI/lib64:/gpfslocalsys/cuda/10.1.2/lib64:/gpfslocalsys/cuda/10.1.2/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/10.1.2/targets/x86_64-linux/lib:/gpfslocalsup/spack_soft/boost/1.70.0/intel-19.0.4-5zoh2xvpvjl3ecgofb5t75zy2tgasuxd/lib:/gpfslocalsys/intel/parallel_studio_xe_2019_update4_cluster_edition/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin:/gpfslocalsup/spack_soft/flatbuffers/1.11.0/gcc-9.1.0-jtgrepiqbrbzlsjawqlmprdmcqp5drqu/lib64:/gpfslocalsup/spack_soft/gcc/9.1.0/gcc-8.3.1-dsq3humdshff2skbethmwa2pg4s2f7rz/lib64:/gpfslocalsup/spack_soft/gcc/9.1.0/gcc-8.3.1-dsq3humdshff2skbethmwa2pg4s2f7rz/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2021-04-02 10:41:22.301759: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfswork/idris/hpe/shpe033/horovod-v0.21.1/horovod/tensorflow:/gpfslocalsup/spack_soft/boost/1.70.0/gcc-8.3.1-ac7s6bl2rvxgv7kt5dhqujijptqob26c/lib:/gpfswork/idris/hpe/shpe033/meshtensorflow_project/boost_nccl:/gpfswork/idris/hpe/shpe033/meshtensorflow_project/install_env/cuda:/gpfslocalsup/spack_soft/openmpi/4.0.2/intel-19.0.4-q6fk6qb3intsc3raxyvu6x3as6uadzsl/lib:/gpfslocalsup/spack_soft/cudnn/7.6.5.32-10.1-linux-x64/gcc-4.8.5-fn27wz3xidimpfcu4t3ctvc6vxjr3afy/lib64:/gpfslocalsup/spack_soft/nccl/2.5.6-2/gcc-4.8.5-qtdldqth7z3ybxfozhgrjryw6c2ideaw/lib:/gpfslocalsys/cuda/10.1.2/nvvm/lib64:/gpfslocalsys/cuda/10.1.2/extras/CUPTI/lib64:/gpfslocalsys/cuda/10.1.2/lib64:/gpfslocalsys/cuda/10.1.2/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/10.1.2/targets/x86_64-linux/lib:/gpfslocalsup/spack_soft/boost/1.70.0/intel-19.0.4-5zoh2xvpvjl3ecgofb5t75zy2tgasuxd/lib:/gpfslocalsys/intel/parallel_studio_xe_2019_update4_cluster_edition/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin:/gpfslocalsup/spack_soft/flatbuffers/1.11.0/gcc-9.1.0-jtgrepiqbrbzlsjawqlmprdmcqp5drqu/lib64:/gpfslocalsup/spack_soft/gcc/9.1.0/gcc-8.3.1-dsq3humdshff2skbethmwa2pg4s2f7rz/lib64:/gpfslocalsup/spack_soft/gcc/9.1.0/gcc-8.3.1-dsq3humdshff2skbethmwa2pg4s2f7rz/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2021-04-02 10:41:22.353358: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-02 10:41:22.397786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-02 10:41:22.434228: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-02 10:41:22.434853: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfswork/idris/hpe/shpe033/horovod-v0.21.1/horovod/tensorflow:/gpfslocalsup/spack_soft/boost/1.70.0/gcc-8.3.1-ac7s6bl2rvxgv7kt5dhqujijptqob26c/lib:/gpfswork/idris/hpe/shpe033/meshtensorflow_project/boost_nccl:/gpfswork/idris/hpe/shpe033/meshtensorflow_project/install_env/cuda:/gpfslocalsup/spack_soft/openmpi/4.0.2/intel-19.0.4-q6fk6qb3intsc3raxyvu6x3as6uadzsl/lib:/gpfslocalsup/spack_soft/cudnn/7.6.5.32-10.1-linux-x64/gcc-4.8.5-fn27wz3xidimpfcu4t3ctvc6vxjr3afy/lib64:/gpfslocalsup/spack_soft/nccl/2.5.6-2/gcc-4.8.5-qtdldqth7z3ybxfozhgrjryw6c2ideaw/lib:/gpfslocalsys/cuda/10.1.2/nvvm/lib64:/gpfslocalsys/cuda/10.1.2/extras/CUPTI/lib64:/gpfslocalsys/cuda/10.1.2/lib64:/gpfslocalsys/cuda/10.1.2/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/10.1.2/targets/x86_64-linux/lib:/gpfslocalsup/spack_soft/boost/1.70.0/intel-19.0.4-5zoh2xvpvjl3ecgofb5t75zy2tgasuxd/lib:/gpfslocalsys/intel/parallel_studio_xe_2019_update4_cluster_edition/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin:/gpfslocalsup/spack_soft/flatbuffers/1.11.0/gcc-9.1.0-jtgrepiqbrbzlsjawqlmprdmcqp5drqu/lib64:/gpfslocalsup/spack_soft/gcc/9.1.0/gcc-8.3.1-dsq3humdshff2skbethmwa2pg4s2f7rz/lib64:/gpfslocalsup/spack_soft/gcc/9.1.0/gcc-8.3.1-dsq3humdshff2skbethmwa2pg4s2f7rz/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2021-04-02 10:41:22.435452: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfswork/idris/hpe/shpe033/horovod-v0.21.1/horovod/tensorflow:/gpfslocalsup/spack_soft/boost/1.70.0/gcc-8.3.1-ac7s6bl2rvxgv7kt5dhqujijptqob26c/lib:/gpfswork/idris/hpe/shpe033/meshtensorflow_project/boost_nccl:/gpfswork/idris/hpe/shpe033/meshtensorflow_project/install_env/cuda:/gpfslocalsup/spack_soft/openmpi/4.0.2/intel-19.0.4-q6fk6qb3intsc3raxyvu6x3as6uadzsl/lib:/gpfslocalsup/spack_soft/cudnn/7.6.5.32-10.1-linux-x64/gcc-4.8.5-fn27wz3xidimpfcu4t3ctvc6vxjr3afy/lib64:/gpfslocalsup/spack_soft/nccl/2.5.6-2/gcc-4.8.5-qtdldqth7z3ybxfozhgrjryw6c2ideaw/lib:/gpfslocalsys/cuda/10.1.2/nvvm/lib64:/gpfslocalsys/cuda/10.1.2/extras/CUPTI/lib64:/gpfslocalsys/cuda/10.1.2/lib64:/gpfslocalsys/cuda/10.1.2/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/10.1.2/targets/x86_64-linux/lib:/gpfslocalsup/spack_soft/boost/1.70.0/intel-19.0.4-5zoh2xvpvjl3ecgofb5t75zy2tgasuxd/lib:/gpfslocalsys/intel/parallel_studio_xe_2019_update4_cluster_edition/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin:/gpfslocalsup/spack_soft/flatbuffers/1.11.0/gcc-9.1.0-jtgrepiqbrbzlsjawqlmprdmcqp5drqu/lib64:/gpfslocalsup/spack_soft/gcc/9.1.0/gcc-8.3.1-dsq3humdshff2skbethmwa2pg4s2f7rz/lib64:/gpfslocalsup/spack_soft/gcc/9.1.0/gcc-8.3.1-dsq3humdshff2skbethmwa2pg4s2f7rz/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2021-04-02 10:41:22.435482: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-04-02 10:41:22.435937: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-02 10:41:22.436654: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-02 10:41:22.436692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-02 10:41:22.436702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      
[2021-04-02 10:41:22.438137: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1477] [Entered horovod_rank()]: 
tensorflow/mpi_ops.py , alltoall() , rank = 1

get_process_groups in basics.py entered +++++

get_process_groups in basics.py after wg +++++

get_process_groups in basics.py after pgs  +++++

get_process_groups in basics.py after insertion in pgs  +++++

tensorflow/mpi_ops.py , alltoall() , process_groups = [[0, 1, 2, 3], [1, 2], [2, 3, 0]]
tensorflow/mpi_ops.py , alltoall() rank = 1 belongs to  process_group = 0
tensorflow/mpi_ops.py , alltoall() , name = None
[2021-04-02 10:41:22.526010: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1898] [EnqueueTensorAlltoall() start, device = -1]: 
[2021-04-02 10:41:22.526257: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1951] [Rank 1, Enqueued tensor : HorovodAlltoall]: 
[2021-04-02 10:41:27.287228: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1144] [RunLoopOnce(), entered, state.num_nccl_streams = 2]: 
[2021-04-02 10:41:27.287315: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1161] [RunLoopOnce(), in loop for nccl_stream = 0]: 
[2021-04-02 10:41:27.287388: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:82] [ComputeResponseList() entered, size_ = 4]: 
[2021-04-02 10:41:27.287442: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:88] [ComputeResponseList(), index_controller = 0, GetRank() = 1]: 
[2021-04-02 10:41:27.287494: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:92] [ComputeResponseList(), GetRanks()[0] = 0]: 
[2021-04-02 10:41:27.287545: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:92] [ComputeResponseList(), GetRanks()[1] = 1]: 
[2021-04-02 10:41:27.287596: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:92] [ComputeResponseList(), GetRanks()[2] = 2]: 
[2021-04-02 10:41:27.287647: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:92] [ComputeResponseList(), GetRanks()[3] = 3]: 
[2021-04-02 10:41:27.287718: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:104] [MPIController::ComputeResponseList(), MPI_Gather TEST 1 OK]: 
[2021-04-02 10:41:27.287770: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:106] [MPIController::ComputeResponseList() TEST 1  recvcounts1[i] = 0]: 
[2021-04-02 10:41:27.287822: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:106] [MPIController::ComputeResponseList() TEST 1  recvcounts1[i] = 0]: 
[2021-04-02 10:41:27.287874: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:106] [MPIController::ComputeResponseList() TEST 1  recvcounts1[i] = 0]: 
[2021-04-02 10:41:27.287925: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:106] [MPIController::ComputeResponseList() TEST 1  recvcounts1[i] = 0]: 
[2021-04-02 10:41:27.287983: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:109] [ComputeResponseList(),  before IsAutoTuning, response_cache.capacity_ = 1024]: 
[2021-04-02 10:41:27.288043: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:110] [ComputeResponseList(),  before IsAutoTuning, parameter_manager.CacheEnabled = 1]: 
[2021-04-02 10:41:27.288095: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:117] [ComputeResponseList(),  IsAutoTuning() = FALSE]: 
[2021-04-02 10:41:27.288147: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:118] [ComputeResponseList(),  after IsAutoTuning, response_cache.capacity_ = 1024]: 
[2021-04-02 10:41:27.288198: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:124] [ComputeResponseList(),  before CacheCoordinator, num_active_bits = 0]: 
[2021-04-02 10:41:27.288251: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:131] [ComputeResponseList(), message_queue_tmp loop]: 
[2021-04-02 10:41:27.288302: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:141] [ComputeResponseList(), message_queue_tmp loop, response_cache.capacity() > 0]: 
[2021-04-02 10:41:27.288355: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:163] [ComputeResponseList() after analyzing message_queue_tmp , GetRank() = 1]: 
[2021-04-02 10:41:27.288407: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:164] [ComputeResponseList() after analyzing message_queue_tmp , capacity = 1024]: 
[2021-04-02 10:41:27.288460: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:176] [ComputeResponseList() should_shut_down = 0]: 
[2021-04-02 10:41:27.288511: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:179] [ComputeResponseList() before ShouldPerformCheck]: 
[2021-04-02 10:41:27.288576: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:186] [ComputeResponseList() ShouldPerformCheck, NOT is_coordinator]: 
[2021-04-02 10:41:27.288628: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:189] [ComputeResponseList() ShoudPerformCheck, response_cache.capacity > 0]: 
[2021-04-02 10:41:27.288687: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:315] [CacheCoordinator::sync() set_should_shut_down() : 0]: 
[2021-04-02 10:41:27.288739: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:196] [ComputeResponseList() after UpdateCheckTime, should_shut_down = 0]: 
[2021-04-02 10:41:27.288790: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:199] [ComputeResponseList() response_cache_.capacity > 0]: 
[2021-04-02 10:41:27.288841: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:894] [Controller::CoordinateCacheAndState entered.]: 
[2021-04-02 10:41:27.288892: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:350] [CacheCoordinator::sync() start.]: 
[2021-04-02 10:41:27.288944: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:369] [CacheCoordinator::sync() after bitvector.]: 
[2021-04-02 10:41:27.288999: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:374] [CacheCoordinator::sync() !should_shut_down.]: 
[2021-04-02 10:41:27.289051: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:386] [CacheCoordinator::sync() before cache_hits.erase.]: 
[2021-04-02 10:41:27.289102: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:390] [CacheCoordinator::sync() after cache_hits.erase.]: 
[2021-04-02 10:41:27.289153: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:406] [CacheCoordinator::sync() before CrossRankBitwiseAnd.]: 
[2021-04-02 10:41:27.289205: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:101] [MPIController::CrossRankBitwiseAnd() start, count = 2, bitvector.size = 2]: 
[2021-04-02 10:41:27.289257: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:103] [MPIController::CrossRankBitwiseAnd() for i = 0, bitvector.data = 5]: 
[2021-04-02 10:41:27.289308: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:103] [MPIController::CrossRankBitwiseAnd() for i = 1, bitvector.data = -1]: 
[2021-04-02 10:41:27.289359: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:107] [MPIController::CrossRankBitwiseAnd() mpi_comm = horovod_global.mpi_world_comm]: 
[2021-04-02 10:41:27.293730: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:114] [MPIController::CrossRankBitwiseAnd() after MPI_Allreduce.]: 
[2021-04-02 10:41:27.293853: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:119] [MPIController::CrossRankBitwiseAnd() end.]: 
[2021-04-02 10:41:27.293934: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:408] [CacheCoordinator::sync() after CrossRankBitwiseAnd.]: 
[2021-04-02 10:41:27.294366: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:484] [CacheCoordinator::sync() end.]: 
[2021-04-02 10:41:27.294445: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:897] [Controller::CoordinateCacheAndState, after cache_coordinator.sync.]: 
[2021-04-02 10:41:27.294524: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:905] [Controller::CoordinateCacheAndState, after invalid_bits.empty.]: 
[2021-04-02 10:41:27.294629: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:909] [Controller::CoordinateCacheAndState, timeline_enabled]: 
[2021-04-02 10:41:27.294726: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:928] [Controller::CoordinateCacheAndState, ended]: 
[2021-04-02 10:41:27.294807: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:205] [ComputeResponseList() after CoordinateCacheAndState()]: 
[2021-04-02 10:41:27.294918: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:211] [ComputeResponseList() num_messages = 1]: 
[2021-04-02 10:41:27.295040: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:213] [ComputeResponseList() start loop for message = 0]: 
[2021-04-02 10:41:27.295183: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:230] [ComputeResponseList() cached != ::HIT for message 0]: 
[2021-04-02 10:41:27.295263: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:234] [ComputeResponseList() cached != ::HIT end for message 0]: 
[2021-04-02 10:41:27.295343: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:237] [ComputeResponseList() end loop for message = 0]: 
[2021-04-02 10:41:27.295454: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:243] [ComputeResponseList() after test response_cache_.capacity ]: 
[2021-04-02 10:41:27.295527: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:246] [1]: ComputeResponseList() Sent 1 messages to coordinator.
[2021-04-02 10:41:27.295658: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:252] [ComputeResponseList() before set_shutdownn should_shut_down = 0]: 
[2021-04-02 10:41:27.295750: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:257] [ComputeResponseList() before computing need_communication, uncached_in_queue = 1]: 
[2021-04-02 10:41:27.295866: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:320] [ComputeResponseList() NOT !need_communication]: 
[2021-04-02 10:41:27.295990: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:525] ComputeResponseList() is_coordinator_ = FALSE, before SendReadyTensors and RecvFinalTensors
[2021-04-02 10:41:27.296076: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:536] [MPIController::ComputeResponseList(), MPI_Gather TEST 3 OK]: 
[2021-04-02 10:41:27.296169: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:538] [MPIController::ComputeResponseList() TEST 3  recvcounts2[i] = 0]: 
[2021-04-02 10:41:27.296245: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:538] [MPIController::ComputeResponseList() TEST 3  recvcounts2[i] = 0]: 
[2021-04-02 10:41:27.296361: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:538] [MPIController::ComputeResponseList() TEST 3  recvcounts2[i] = 0]: 
[2021-04-02 10:41:27.296472: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:538] [MPIController::ComputeResponseList() TEST 3  recvcounts2[i] = 0]: 
[2021-04-02 10:41:27.296554: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:542] [ComputeResponseList() is_coordinator = FALSE , before message_queuue.empty() , setting should_shut_down = 0]: 
[2021-04-02 10:41:27.296634: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:544] ComputeResponseList() is_coordinator_ = FALSE, Loop before add_request
[2021-04-02 10:41:27.296742: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:546] ComputeResponseList() is_coordinator_ = FALSE, Loop before pop_front
[2021-04-02 10:41:27.296854: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:548] ComputeResponseList() is_coordinator_ = FALSE, Loop after pop_front
[2021-04-02 10:41:27.296979: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:551] ComputeResponseList() is_coordinator_ = FALSE, before SendReadyTensors
[2021-04-02 10:41:27.297088: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:231] [MPIController::SendReadyTensors(), started, controller_index = 0]: 
[2021-04-02 10:41:27.297207: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:234] [MPIController::SendReadyTensors(), after SerializeToString, message_list.size = 1]: 
[2021-04-02 10:41:27.297304: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:236] [MPIController::SendReadyTensors(), before MPI_Gather, encoded_message_length = 137]: 
[2021-04-02 10:41:27.297379: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:240] [MPIController::SendReadyTensors(), before MPI_Gather, mpi_comm = horovod_global.mpi_world_comm]: 
[2021-04-02 10:41:27.297480: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:261] [MPIController::SendReadyTensors(), after MPI_Gatherv.]: 
[2021-04-02 10:41:27.297558: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:266] [MPIController::SendReadyTensors(), ended.]: 
[2021-04-02 10:41:27.297656: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:555] ComputeResponseList() is_coordinator_ = FALSE, before RecvFinalTensors
[2021-04-02 10:41:27.298856: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:270] MPIController::RecvFinalTensors(), started .
[2021-04-02 10:41:27.301425: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:274] MPIController::RecvFinalTensors(), after MPI_Bcast 1.
[2021-04-02 10:41:27.301538: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:281] MPIController::RecvFinalTensors(), before MPI_Bcast 2.
[2021-04-02 10:41:27.301627: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:284] MPIController::RecvFinalTensors(), after MPI_Bcast 2.
[2021-04-02 10:41:27.301709: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:289] [MPIController::RecvFinalTensors(), before ParseFromBytes.]: 
[2021-04-02 10:41:27.301795: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:291] [MPIController::RecvFinalTensors(), after ParseFromBytes.]: 
[2021-04-02 10:41:27.301870: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:293] [MPIController::RecvFinalTensors(), ended.]: 
[2021-04-02 10:41:27.303495: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:558] ComputeResponseList() is_coordinator_ = FALSE, after SendReadyTensors and RecvFinalTensors0
[2021-04-02 10:41:27.303588: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:563] ComputeResponseList() !response_list.responses().empty()
[2021-04-02 10:41:27.303668: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:568] ComputeResponseList() Sending ready responses as HorovodAlltoall; 
[2021-04-02 10:41:27.303763: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:589] ComputeResponseList() ending.
[2021-04-02 10:41:27.303845: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1170] [RunLoopOnce(), for nccl_stream = 0, finished ComputeResponseList]: 
[2021-04-02 10:41:27.303989: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1191] [1]: Performing HorovodAlltoall
[2021-04-02 10:41:27.304065: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1192] [1]: Processing 1 tensors
[2021-04-02 10:41:27.304175: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:412] [PerformOperation for iComm = 0]: 
[2021-04-02 10:41:27.304309: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:478] [PerformOperation, before ExecuteOperation.]: 
[2021-04-02 10:41:27.304388: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/operation_manager.cc:130] [OperationManager::ExecuteOperation() start]: 
[2021-04-02 10:41:27.304647: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/operation_manager.cc:138] [OperationManager::ExecuteOperation() Response::ALLTOALL]: 
[2021-04-02 10:41:27.304740: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/operation_manager.cc:95] [OperationManager::ExecuteAlltoall() entered]: 
[2021-04-02 10:41:27.304873: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/operation_manager.cc:97] [OperationManager::ExecuteAlltoall() in loop]: 
[2021-04-02 10:41:27.304979: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/nccl_operations.cc:598] [NCCLAlltoall::Enabled()]: 
[2021-04-02 10:41:27.305058: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/operation_manager.cc:99] [OperationManager::ExecuteAlltoall() op->Enabled()]: 
[2021-04-02 10:41:27.305133: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/nccl_operations.cc:611] [NCCLAlltoall::Execute() start.]: 
[2021-04-02 10:41:27.305311: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/nccl_operations.cc:613] [NCCLAlltoall::Execute() NCCL_P2P_SUPPORTED.]: 
[2021-04-02 10:41:27.305409: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/nccl_operations.cc:617] [NCCLAlltoall::Execute() before InitGPU]: 
[2021-04-02 10:41:27.305517: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:40] [GPUOpContext::InitGPU() entered.]: 
[2021-04-02 10:41:27.305617: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/gpu_operations.cc:42] [GPUOpContext::InitGPU() device = -1]: 
[2021-04-02 10:41:27.305697: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/ops/cuda_operations.cc:136] [SetDevice() cuda for device = -1 start.]: 
[2021-04-02 10:41:27.306361: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:482] [1]: ExecuteOperation Failed
[2021-04-02 10:41:27.306464: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:485] [PerformOperation, after ExecuteOperation.]: 
[2021-04-02 10:41:27.306624: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:496] [PerformOperation, after check in_progress]: 
[2021-04-02 10:41:27.306706: 
PYTHON : Exception in hvd.alltoall.
T
 /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1195] [1]: Finished performing HorovodAlltoall
[2021-04-02 10:41:27
HorovodBasics.shutdown
.
306851: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:[2021-04-02 10:41:27.306912: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1427] [horovod_shutdown() start.]: 
1161] [RunLoopOnce(), in loop for nccl_stream = 1]: 
[2021-04-02 10:41:27.307059: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:82] [ComputeResponseList() entered, size_ = 2]: 
[2021-04-02 10:41:27.307139: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:88] [ComputeResponseList(), index_controller = 1, GetRank() = 0]: 
[2021-04-02 10:41:27.307246: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:92] [ComputeResponseList(), GetRanks()[0] = 1]: 
[2021-04-02 10:41:27.307321: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:92] [ComputeResponseList(), GetRanks()[1] = 2]: 
[2021-04-02 10:41:27.307505: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:104] [MPIController::ComputeResponseList(), MPI_Gather TEST 1 OK]: 
[2021-04-02 10:41:27.307582: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:106] [MPIController::ComputeResponseList() TEST 1  recvcounts1[i] = 1]: 
[2021-04-02 10:41:27.307684: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:106] [MPIController::ComputeResponseList() TEST 1  recvcounts1[i] = 2]: 
[2021-04-02 10:41:27.307760: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:106] [MPIController::ComputeResponseList() TEST 1  recvcounts1[i] = 0]: 
[2021-04-02 10:41:27.307861: I /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:106] [MPIController::ComputeResponseList() TEST 1  recvcounts1[i] = 0]: 
[2021-04-02 10:41:27.309173: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:109] [ComputeResponseList(),  before IsAutoTuning, response_cache.capacity_ = 1024]: 
[2021-04-02 10:41:27.309267: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:110] [ComputeResponseList(),  before IsAutoTuning, parameter_manager.CacheEnabled = 1]: 
[2021-04-02 10:41:27.309388: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:117] [ComputeResponseList(),  IsAutoTuning() = FALSE]: 
[2021-04-02 10:41:27.309726: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:118] [ComputeResponseList(),  after IsAutoTuning, response_cache.capacity_ = 1024]: 
[2021-04-02 10:41:27.310255: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:124] [ComputeResponseList(),  before CacheCoordinator, num_active_bits = 0]: 
[2021-04-02 10:41:27.310334: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:163] [ComputeResponseList() after analyzing message_queue_tmp , GetRank() = 0]: 
[2021-04-02 10:41:27.310413: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:164] [ComputeResponseList() after analyzing message_queue_tmp , capacity = 1024]: 
[2021-04-02 10:41:27.310490: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:176] [ComputeResponseList() should_shut_down = 1]: 
[2021-04-02 10:41:27.310613: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:179] [ComputeResponseList() before ShouldPerformCheck]: 
[2021-04-02 10:41:27.310770: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:183] [ComputeResponseList() ShouldPerformCheck, is_coordinator, should_shut_down = 1]: 
[2021-04-02 10:41:27.310870: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:189] [ComputeResponseList() ShoudPerformCheck, response_cache.capacity > 0]: 
[2021-04-02 10:41:27.310981: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:315] [CacheCoordinator::sync() set_should_shut_down() : 1]: 
[2021-04-02 10:41:27.311064: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:196] [ComputeResponseList() after UpdateCheckTime, should_shut_down = 1]: 
[2021-04-02 10:41:27.311169: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:199] [ComputeResponseList() response_cache_.capacity > 0]: 
[2021-04-02 10:41:27.311268: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:894] [Controller::CoordinateCacheAndState entered.]: 
[2021-04-02 10:41:27.311347: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:350] [CacheCoordinator::sync() start.]: 
[2021-04-02 10:41:27.311426: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:369] [CacheCoordinator::sync() after bitvector.]: 
[2021-04-02 10:41:27.311553: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:377] [CacheCoordinator::sync() should_shut_down.]: 
[2021-04-02 10:41:27.311662: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:386] [CacheCoordinator::sync() before cache_hits.erase.]: 
[2021-04-02 10:41:27.311738: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:390] [CacheCoordinator::sync() after cache_hits.erase.]: 
[2021-04-02 10:41:27.311869: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:406] [CacheCoordinator::sync() before CrossRankBitwiseAnd.]: 
[2021-04-02 10:41:27.312027: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:101] [MPIController::CrossRankBitwiseAnd() start, count = 2, bitvector.size = 2]: 
[2021-04-02 10:41:27.312127: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:103] [MPIController::CrossRankBitwiseAnd() for i = 0, bitvector.data = 6]: 
[2021-04-02 10:41:27.312218: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:103] [MPIController::CrossRankBitwiseAnd() for i = 1, bitvector.data = -1]: 
[2021-04-02 10:41:27.312305: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:109] [MPIController::CrossRankBitwiseAnd() mpi_comm ! MPI_COMM_NULL and  mpi_comm ! MPI_COMM_WORLD ]: 
[2021-04-02 10:41:27.312455: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:114] [MPIController::CrossRankBitwiseAnd() after MPI_Allreduce.]: 
[2021-04-02 10:41:27.312532: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/mpi/mpi_controller.cc:119] [MPIController::CrossRankBitwiseAnd() end.]: 
[2021-04-02 10:41:27.312612: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:408] [CacheCoordinator::sync() after CrossRankBitwiseAnd.]: 
[2021-04-02 10:41:27.312691: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:427] [CacheCoordinator::sync() setting should_shut_down = true.]: 
[2021-04-02 10:41:27.312772: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/response_cache.cc:484] [CacheCoordinator::sync() end.]: 
[2021-04-02 10:41:27.312943: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:897] [Controller::CoordinateCacheAndState, after cache_coordinator.sync.]: 
[2021-04-02 10:41:27.313191: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:905] [Controller::CoordinateCacheAndState, after invalid_bits.empty.]: 
[2021-04-02 10:41:27.313309: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:909] [Controller::CoordinateCacheAndState, timeline_enabled]: 
[2021-04-02 10:41:27.313413: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:928] [Controller::CoordinateCacheAndState, ended]: 
[2021-04-02 10:41:27.313523: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:205] [ComputeResponseList() after CoordinateCacheAndState()]: 
[2021-04-02 10:41:27.313600: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:211] [ComputeResponseList() num_messages = 0]: 
[2021-04-02 10:41:27.313680: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:243] [ComputeResponseList() after test response_cache_.capacity ]: 
[2021-04-02 10:41:27.313762: T /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:250] [0]: ComputeResponseList() message_queue_empty, No message sent to coordinator.
[2021-04-02 10:41:27.313865: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:252] [ComputeResponseList() before set_shutdownn should_shut_down = 1]: 
[2021-04-02 10:41:27.313943: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:257] [ComputeResponseList() before computing need_communication, uncached_in_queue = 0]: 
[2021-04-02 10:41:27.314026: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:264] [ComputeResponseList() need_communication = FALSE]: 
[2021-04-02 10:41:27.314176: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/controller.cc:267] [ComputeResponseList() cache_coordinator.cache_hits().empty(), return]: 
[2021-04-02 10:41:27.314289: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:1170] [RunLoopOnce(), for nccl_stream = 1, finished ComputeResponseList]: 
[2021-04-02 10:41:27.314411: D /gpfsdswork/projects/idris/hpe/shpe033/horovod-v0.21.1/horovod/common/operations.cc:875] [1]: Shutting down background thread
